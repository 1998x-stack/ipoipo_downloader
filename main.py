"""
ä¸»ç¨‹åº - IPOæŠ¥å‘Šä¸‹è½½å™¨
"""
import sys
import argparse
from utils.logger import get_logger
from core.proxy_manager import ProxyManager
from core.http_client import HTTPClient
from core.database import Database
from download.file_manager import FileManager
from scrapers.category_scraper import CategoryScraper
from scrapers.list_scraper import ListScraper
from scrapers.download_scraper import DownloadScraper
from download.downloader import Downloader
from config.settings import USE_PROXY

logger = get_logger(__name__)


class IPODownloader:
    """IPOæŠ¥å‘Šä¸‹è½½å™¨ä¸»ç±»"""
    
    def __init__(self, use_proxy: bool = USE_PROXY):
        logger.info("ğŸš€ åˆå§‹åŒ–IPOæŠ¥å‘Šä¸‹è½½å™¨...")
        
        # åˆå§‹åŒ–ä»£ç†ç®¡ç†å™¨
        self.proxy_manager = None
        if use_proxy:
            try:
                self.proxy_manager = ProxyManager()
                logger.info("â³ æµ‹è¯•ä»£ç†èŠ‚ç‚¹...")
                self.proxy_manager.test_all_nodes()
            except Exception as e:
                logger.error(f"âŒ ä»£ç†åˆå§‹åŒ–å¤±è´¥: {e}")
                logger.warning("âš ï¸ å°†ä¸ä½¿ç”¨ä»£ç†ç»§ç»­è¿è¡Œ")
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.client = HTTPClient(
            use_proxy=use_proxy and self.proxy_manager is not None,
            proxy_manager=self.proxy_manager
        )
        self.db = Database()
        self.fm = FileManager()
        
        # åˆå§‹åŒ–çˆ¬è™«
        self.category_scraper = CategoryScraper(self.client, self.db)
        self.list_scraper = ListScraper(self.client, self.db)
        self.download_scraper = DownloadScraper(self.client, self.db)
        self.downloader = Downloader(self.client, self.db, self.fm)
        
        logger.info("âœ… åˆå§‹åŒ–å®Œæˆï¼\n")
    
    def stage1_scrape_categories(self):
        """Stage 1: çˆ¬å–åˆ†ç±»"""
        self.category_scraper.scrape_all_categories()
    
    def stage2_scrape_lists(self, max_pages: int = None, categories: list = None):
        """Stage 2: çˆ¬å–æŠ¥å‘Šåˆ—è¡¨"""
        if categories:
            # çˆ¬å–æŒ‡å®šåˆ†ç±»
            for category_id in categories:
                category = self.db.get_all_categories()
                category = [c for c in category if c['category_id'] == category_id]
                if category:
                    self.list_scraper.scrape_category(
                        category[0]['category_id'],
                        category[0]['category_name'],
                        max_pages=max_pages
                    )
        else:
            # çˆ¬å–æ‰€æœ‰åˆ†ç±»
            self.list_scraper.scrape_all_categories(max_pages_per_category=max_pages)
    
    def stage3_get_download_urls(self, limit: int = None):
        """Stage 3 & 4: è·å–ä¸‹è½½é“¾æ¥"""
        self.download_scraper.process_all_pending_reports(limit=limit or 100)
    
    def stage4_download_reports(self, max_reports: int = None, 
                               category: str = None,
                               force: bool = False,
                               concurrent: bool = False):
        """Stage 5: ä¸‹è½½æŠ¥å‘Š"""
        if category:
            self.downloader.download_reports_by_category(
                category, 
                max_reports=max_reports,
                force=force
            )
        else:
            self.downloader.download_all_reports(
                max_reports=max_reports,
                force=force,
                use_concurrent=concurrent
            )
    
    def run_full_pipeline(self, max_pages: int = None, 
                          max_reports: int = None,
                          categories: list = None):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æµç¨‹")
        logger.info("=" * 60)
        
        # Stage 1: çˆ¬å–åˆ†ç±»
        self.stage1_scrape_categories()
        
        # Stage 2: çˆ¬å–æŠ¥å‘Šåˆ—è¡¨
        self.stage2_scrape_lists(max_pages=max_pages, categories=categories)
        
        # Stage 3 & 4: è·å–ä¸‹è½½é“¾æ¥
        self.stage3_get_download_urls()
        
        # Stage 5: ä¸‹è½½æŠ¥å‘Š
        self.stage4_download_reports(max_reports=max_reports)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        self.show_stats()
    
    def show_stats(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        stats = self.db.get_stats()
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        logger.info("=" * 60)
        logger.info(f"åˆ†ç±»æ•°é‡: {stats['total_categories']}")
        logger.info(f"æŠ¥å‘Šæ€»æ•°: {stats['total_reports']}")
        logger.info(f"\næŒ‰çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in stats.get('reports_by_status', {}).items():
            logger.info(f"  - {status}: {count}")
        logger.info(f"\nä¸‹è½½ç»Ÿè®¡:")
        logger.info(f"  - å·²å®Œæˆ: {stats.get('downloads_completed', 0)}")
        logger.info(f"  - å¤±è´¥: {stats.get('downloads_failed', 0)}")
        logger.info("=" * 60)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")
        self.client.close()
        self.db.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="IPOæŠ¥å‘Šä¸‹è½½å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆæ¯ä¸ªåˆ†ç±»åªçˆ¬å‰2é¡µï¼Œæœ€å¤šä¸‹è½½10ä¸ªæŠ¥å‘Šï¼‰
  python main.py --full --max-pages 2 --max-reports 10
  
  # åªçˆ¬å–åˆ†ç±»
  python main.py --stage1
  
  # åªçˆ¬å–æŠ¥å‘Šåˆ—è¡¨ï¼ˆæ‰€æœ‰åˆ†ç±»ï¼Œæ¯ä¸ª5é¡µï¼‰
  python main.py --stage2 --max-pages 5
  
  # åªè·å–ä¸‹è½½é“¾æ¥ï¼ˆå‰50ä¸ªï¼‰
  python main.py --stage3 --limit 50
  
  # åªä¸‹è½½æŠ¥å‘Šï¼ˆæœ€å¤š20ä¸ªï¼Œä½¿ç”¨å¹¶å‘ï¼‰
  python main.py --stage4 --max-reports 20 --concurrent
  
  # ä¸‹è½½æŒ‡å®šåˆ†ç±»çš„æŠ¥å‘Š
  python main.py --stage4 --category 34 --max-reports 10
  
  # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  python main.py --stats
  
  # ä¸ä½¿ç”¨ä»£ç†
  python main.py --full --no-proxy
        """
    )
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument('--full', action='store_true', 
                       help='è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆæ‰€æœ‰stageï¼‰')
    parser.add_argument('--stage1', action='store_true',
                       help='åªè¿è¡ŒStage 1: çˆ¬å–åˆ†ç±»')
    parser.add_argument('--stage2', action='store_true',
                       help='åªè¿è¡ŒStage 2: çˆ¬å–æŠ¥å‘Šåˆ—è¡¨')
    parser.add_argument('--stage3', action='store_true',
                       help='åªè¿è¡ŒStage 3: è·å–ä¸‹è½½é“¾æ¥')
    parser.add_argument('--stage4', action='store_true',
                       help='åªè¿è¡ŒStage 4: ä¸‹è½½æŠ¥å‘Š')
    parser.add_argument('--stats', action='store_true',
                       help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    
    # å‚æ•°
    parser.add_argument('--max-pages', type=int,
                       help='æ¯ä¸ªåˆ†ç±»æœ€å¤šçˆ¬å–çš„é¡µæ•°')
    parser.add_argument('--max-reports', type=int,
                       help='æœ€å¤šä¸‹è½½çš„æŠ¥å‘Šæ•°')
    parser.add_argument('--limit', type=int,
                       help='å¤„ç†çš„æŠ¥å‘Šæ•°é™åˆ¶')
    parser.add_argument('--category', type=str,
                       help='æŒ‡å®šåˆ†ç±»IDï¼ˆä¾‹å¦‚: 34ï¼‰')
    parser.add_argument('--categories', type=str, nargs='+',
                       help='æŒ‡å®šå¤šä¸ªåˆ†ç±»IDï¼ˆä¾‹å¦‚: 34 69 85ï¼‰')
    
    # é€‰é¡¹
    parser.add_argument('--no-proxy', action='store_true',
                       help='ä¸ä½¿ç”¨ä»£ç†')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„æ–‡ä»¶')
    parser.add_argument('--concurrent', action='store_true',
                       help='ä½¿ç”¨å¹¶å‘ä¸‹è½½')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ“ä½œ
    if not any([args.full, args.stage1, args.stage2, args.stage3, args.stage4, args.stats]):
        parser.print_help()
        sys.exit(0)
    
    # åˆå§‹åŒ–ä¸‹è½½å™¨
    try:
        downloader = IPODownloader(use_proxy=not args.no_proxy)
        
        # æ‰§è¡Œæ“ä½œ
        if args.stats:
            downloader.show_stats()
        
        if args.stage1:
            downloader.stage1_scrape_categories()
        
        if args.stage2:
            downloader.stage2_scrape_lists(
                max_pages=args.max_pages,
                categories=args.categories
            )
        
        if args.stage3:
            downloader.stage3_get_download_urls(limit=args.limit)
        
        if args.stage4:
            downloader.stage4_download_reports(
                max_reports=args.max_reports,
                category=args.category,
                force=args.force,
                concurrent=args.concurrent
            )
        
        if args.full:
            downloader.run_full_pipeline(
                max_pages=args.max_pages,
                max_reports=args.max_reports,
                categories=args.categories
            )
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if not args.stats:
            downloader.show_stats()
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        sys.exit(1)
    finally:
        try:
            downloader.cleanup()
        except:
            pass
    
    logger.info("\nâœ… ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()