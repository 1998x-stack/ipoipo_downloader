"""
ä¸»ç¨‹åº - IPOæŠ¥å‘Šä¸‹è½½å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰

ä¿®å¤å†…å®¹ï¼š
1. ä½¿ç”¨ proxy_manager.select_random() è·å–ä»£ç†
2. HTTPClient ä½¿ç”¨ Session ä¿æŒ cookies
3. æ­£ç¡®å¤„ç† Tengine CDN é˜²ç›—é“¾
"""
import sys
import argparse
import os
import sys
import os
import argparse

# Add src to Python path to import modules
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from utils.logger import get_logger
from model.proxy_manager import ProxyManager
from model.http_client import HTTPClient
from model.database import Database
from downloader.file_manager import FileManager
from scraper.category_scraper import CategoryScraper
from scraper.list_scraper import ListScraper
from scraper.download_scraper import DownloadScraper
from downloader.downloader import Downloader
from config.settings import USE_PROXY

logger = get_logger(__name__)


class IPODownloader:
    """IPOæŠ¥å‘Šä¸‹è½½å™¨ä¸»ç±»"""
    
    def __init__(self, use_proxy: bool = USE_PROXY):
        logger.info("ğŸš€ åˆå§‹åŒ–IPOæŠ¥å‘Šä¸‹è½½å™¨...")
        
        # åˆå§‹åŒ–ä»£ç†ç®¡ç†å™¨
        self.proxy_manager = None
        proxy_url = None
        
        if use_proxy:
            try:
                self.proxy_manager = ProxyManager(use_local_clash=True)
                logger.info("â³ æµ‹è¯•ä»£ç†èŠ‚ç‚¹...")
                self.proxy_manager.test_all_nodes()
                
                # ä½¿ç”¨ select_random() é€‰æ‹©éšæœºèŠ‚ç‚¹
                selected_node = self.proxy_manager.select_random()
                
                # è·å–ä»£ç†URLï¼ˆä½¿ç”¨æœ¬åœ°Clashç«¯å£ï¼‰
                proxy_url = f"http://127.0.0.1:{self.proxy_manager.local_port}"
                logger.info(f"ğŸ“¡ ä½¿ç”¨ä»£ç†: {proxy_url}")
                logger.info(f"ğŸ“¡ å½“å‰èŠ‚ç‚¹: {selected_node.name}")
                
            except Exception as e:
                logger.error(f"âŒ ä»£ç†åˆå§‹åŒ–å¤±è´¥: {e}")
                logger.warning("âš ï¸ å°†ä¸ä½¿ç”¨ä»£ç†ç»§ç»­è¿è¡Œ")
                use_proxy = False
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        # å…³é”®ï¼šä¼ å…¥ proxy_url è®© HTTPClient æ­£ç¡®é…ç½®ä»£ç†
        self.client = HTTPClient(
            use_proxy=use_proxy,
            proxy_manager=self.proxy_manager,
            proxy_url=proxy_url
        )
        self.db = Database()
        self.fm = FileManager()
        
        # åˆå§‹åŒ–çˆ¬è™«
        self.category_scraper = CategoryScraper(self.client, self.db)
        self.list_scraper = ListScraper(self.client, self.db)
        self.download_scraper = DownloadScraper(self.client, self.db)
        
        # åˆå§‹åŒ–ä¸‹è½½å™¨ï¼ˆä¼ å…¥ä»£ç†åˆ‡æ¢å›è°ƒï¼‰
        self.downloader = Downloader(
            self.client, 
            self.db, 
            self.fm,
            proxy_switch_callback=self.switch_proxy_node if use_proxy else None
        )
        
        logger.info("âœ… åˆå§‹åŒ–å®Œæˆï¼\n")
    
    def stage1_scrape_categories(self):
        """Stage 1: çˆ¬å–åˆ†ç±»"""
        self.category_scraper.scrape_all_categories()
    
    def stage2_scrape_lists(self, max_pages: int = None, categories: list = None):
        """Stage 2: çˆ¬å–æŠ¥å‘Šåˆ—è¡¨"""
        if categories:
            # çˆ¬å–æŒ‡å®šåˆ†ç±»
            for category_id in categories:
                category = self.db.get_all_categories()
                category = [c for c in category if c['category_id'] == category_id]
                if category:
                    self.list_scraper.scrape_category(
                        category[0]['category_id'],
                        category[0]['category_name'],
                        max_pages=max_pages
                    )
        else:
            # çˆ¬å–æ‰€æœ‰åˆ†ç±»
            self.list_scraper.scrape_all_categories(max_pages_per_category=max_pages)
    
    def stage3_get_download_urls(self, limit: int = None):
        """Stage 3 & 4: è·å–ä¸‹è½½é“¾æ¥"""
        self.download_scraper.process_all_pending_reports(limit=limit or 100)
    
    def stage4_download_reports(self, max_reports: int = None, 
                               category: str = None,
                               force: bool = False,
                               concurrent: bool = False):
        """Stage 5: ä¸‹è½½æŠ¥å‘Š"""
        if category:
            self.downloader.download_reports_by_category(
                category, 
                max_reports=max_reports,
                force=force
            )
        else:
            self.downloader.download_all_reports(
                max_reports=max_reports,
                force=force,
                use_concurrent=concurrent
            )
    
    def retry_failed(self, max_reports: int = None):
        """é‡è¯•å¤±è´¥çš„ä¸‹è½½"""
        self.downloader.retry_failed_downloads(max_reports=max_reports)
    
    def extract_zips(self, category: str = None, max_files: int = None):
        """è§£å‹å·²ä¸‹è½½çš„ZIPæ–‡ä»¶"""
        self.downloader.extract_downloaded_zips(
            category_name=category, 
            max_files=max_files
        )
    
    def run_full_pipeline(self, max_pages: int = None, 
                          max_reports: int = None,
                          categories: list = None):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æµç¨‹")
        logger.info("=" * 60)
        
        # Stage 1: çˆ¬å–åˆ†ç±»
        self.stage1_scrape_categories()
        
        # Stage 2: çˆ¬å–æŠ¥å‘Šåˆ—è¡¨
        self.stage2_scrape_lists(max_pages=max_pages, categories=categories)
        
        # Stage 3 & 4: è·å–ä¸‹è½½é“¾æ¥
        self.stage3_get_download_urls()
        
        # Stage 5: ä¸‹è½½æŠ¥å‘Š
        self.stage4_download_reports(max_reports=max_reports)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        self.show_stats()
    
    def show_stats(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        stats = self.db.get_stats()
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        logger.info("=" * 60)
        logger.info(f"åˆ†ç±»æ•°é‡: {stats['total_categories']}")
        logger.info(f"æŠ¥å‘Šæ€»æ•°: {stats['total_reports']}")
        logger.info(f"\næŒ‰çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in stats.get('reports_by_status', {}).items():
            logger.info(f"  - {status}: {count}")
        logger.info(f"\nä¸‹è½½ç»Ÿè®¡:")
        logger.info(f"  - å·²å®Œæˆ: {stats.get('downloads_completed', 0)}")
        logger.info(f"  - å¤±è´¥: {stats.get('downloads_failed', 0)}")
        logger.info("=" * 60)
    
    def switch_proxy_node(self) -> bool:
        """
        åˆ‡æ¢ä»£ç†èŠ‚ç‚¹ï¼ˆå½“ä¸‹è½½å¤±è´¥æ—¶è‡ªåŠ¨è°ƒç”¨ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸåˆ‡æ¢
        """
        if not self.proxy_manager:
            logger.warning("âš ï¸ æœªé…ç½®ä»£ç†ç®¡ç†å™¨ï¼Œæ— æ³•åˆ‡æ¢èŠ‚ç‚¹")
            return False
        
        try:
            # æ ‡è®°å½“å‰èŠ‚ç‚¹å¤±è´¥
            if self.proxy_manager.current_node:
                self.proxy_manager.mark_node_failed(self.proxy_manager.current_node)
            
            # é€‰æ‹©æ–°çš„éšæœºèŠ‚ç‚¹
            new_node = self.proxy_manager.select_random()
            logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°æ–°èŠ‚ç‚¹: {new_node.name} ({new_node.latency:.0f}ms)")
            
            # æ›´æ–°HTTPClientçš„ä»£ç†é…ç½®
            proxy_url = f"http://127.0.0.1:{self.proxy_manager.local_port}"
            self.client.session.proxies = {
                'http': proxy_url,
                'https': proxy_url
            }
            
            # æ¸…é™¤æ—§çš„cookiesï¼ˆæ–°èŠ‚ç‚¹å¯èƒ½éœ€è¦é‡æ–°å»ºç«‹ä¼šè¯ï¼‰
            self.client.clear_cookies()
            logger.debug(f"ğŸª å·²æ¸…é™¤cookies")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ‡æ¢èŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    def get_proxy_switch_callback(self):
        """è·å–ä»£ç†åˆ‡æ¢å›è°ƒå‡½æ•°ï¼ˆä¾›Downloaderä½¿ç”¨ï¼‰"""
        return self.switch_proxy_node
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")
        self.client.close()
        self.db.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="IPOæŠ¥å‘Šä¸‹è½½å™¨ï¼ˆä¿®å¤é˜²ç›—é“¾ç‰ˆæœ¬ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆæ¯ä¸ªåˆ†ç±»åªçˆ¬å‰2é¡µï¼Œæœ€å¤šä¸‹è½½10ä¸ªæŠ¥å‘Šï¼‰
  python main.py --full --max-pages 2 --max-reports 10
  
  # åªçˆ¬å–åˆ†ç±»
  python main.py --stage1
  
  # åªçˆ¬å–æŠ¥å‘Šåˆ—è¡¨ï¼ˆæ‰€æœ‰åˆ†ç±»ï¼Œæ¯ä¸ª5é¡µï¼‰
  python main.py --stage2 --max-pages 5
  
  # åªè·å–ä¸‹è½½é“¾æ¥ï¼ˆå‰50ä¸ªï¼‰
  python main.py --stage3 --limit 50
  
  # åªä¸‹è½½æŠ¥å‘Šï¼ˆæœ€å¤š20ä¸ªï¼‰
  python main.py --stage4 --max-reports 20
  
  # ä¸‹è½½æŒ‡å®šåˆ†ç±»çš„æŠ¥å‘Š
  python main.py --stage4 --category 34 --max-reports 10
  
  # é‡è¯•å¤±è´¥çš„ä¸‹è½½
  python main.py --retry --max-reports 10
  
  # è§£å‹å·²ä¸‹è½½çš„ZIPæ–‡ä»¶
  python main.py --extract --max-reports 20
  
  # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  python main.py --stats
  
  # ä¸ä½¿ç”¨ä»£ç†
  python main.py --full --no-proxy

æ³¨æ„äº‹é¡¹:
  - ç¡®ä¿Clashå®¢æˆ·ç«¯æ­£åœ¨è¿è¡Œï¼ˆé»˜è®¤ç«¯å£7890ï¼‰
  - ç¨‹åºä¼šè‡ªåŠ¨é€‰æ‹©éšæœºä»£ç†èŠ‚ç‚¹
  - å¦‚é‡403é”™è¯¯ï¼Œç¨‹åºä¼šæ­£ç¡®è®¾ç½®Refererç»•è¿‡é˜²ç›—é“¾
        """
    )
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument('--full', action='store_true', 
                       help='è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆæ‰€æœ‰stageï¼‰')
    parser.add_argument('--stage1', action='store_true',
                       help='åªè¿è¡ŒStage 1: çˆ¬å–åˆ†ç±»')
    parser.add_argument('--stage2', action='store_true',
                       help='åªè¿è¡ŒStage 2: çˆ¬å–æŠ¥å‘Šåˆ—è¡¨')
    parser.add_argument('--stage3', action='store_true',
                       help='åªè¿è¡ŒStage 3: è·å–ä¸‹è½½é“¾æ¥')
    parser.add_argument('--stage4', action='store_true',
                       help='åªè¿è¡ŒStage 4: ä¸‹è½½æŠ¥å‘Š')
    parser.add_argument('--retry', action='store_true',
                       help='é‡è¯•å¤±è´¥çš„ä¸‹è½½')
    parser.add_argument('--extract', action='store_true',
                       help='è§£å‹å·²ä¸‹è½½çš„ZIPæ–‡ä»¶')
    parser.add_argument('--stats', action='store_true',
                       help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    
    # å‚æ•°
    parser.add_argument('--max-pages', type=int,
                       help='æ¯ä¸ªåˆ†ç±»æœ€å¤šçˆ¬å–çš„é¡µæ•°')
    parser.add_argument('--max-reports', type=int,
                       help='æœ€å¤šä¸‹è½½çš„æŠ¥å‘Šæ•°')
    parser.add_argument('--limit', type=int,
                       help='å¤„ç†çš„æŠ¥å‘Šæ•°é™åˆ¶')
    parser.add_argument('--category', type=str,
                       help='æŒ‡å®šåˆ†ç±»IDï¼ˆä¾‹å¦‚: 34ï¼‰')
    parser.add_argument('--categories', type=str, nargs='+',
                       help='æŒ‡å®šå¤šä¸ªåˆ†ç±»IDï¼ˆä¾‹å¦‚: 34 69 85ï¼‰')
    
    # é€‰é¡¹
    parser.add_argument('--no-proxy', action='store_true',
                       help='ä¸ä½¿ç”¨ä»£ç†')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„æ–‡ä»¶')
    parser.add_argument('--concurrent', action='store_true',
                       help='ä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼ˆå¯èƒ½è§¦å‘æ›´å¤šé˜²æŠ¤ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ“ä½œ
    if not any([args.full, args.stage1, args.stage2, args.stage3, 
                args.stage4, args.retry, args.extract, args.stats]):
        parser.print_help()
        sys.exit(0)
    
    # åˆå§‹åŒ–ä¸‹è½½å™¨
    downloader = None
    try:
        downloader = IPODownloader(use_proxy=not args.no_proxy)
        
        # æ‰§è¡Œæ“ä½œ
        if args.stats:
            downloader.show_stats()
        
        if args.stage1:
            downloader.stage1_scrape_categories()
        
        if args.stage2:
            downloader.stage2_scrape_lists(
                max_pages=args.max_pages,
                categories=args.categories
            )
        
        if args.stage3:
            downloader.stage3_get_download_urls(limit=args.limit)
        
        if args.stage4:
            downloader.stage4_download_reports(
                max_reports=args.max_reports,
                category=args.category,
                force=args.force,
                concurrent=args.concurrent
            )
        
        if args.retry:
            downloader.retry_failed(max_reports=args.max_reports)
        
        if args.extract:
            downloader.extract_zips(category=args.category, max_files=args.max_reports)
        
        if args.full:
            downloader.run_full_pipeline(
                max_pages=args.max_pages,
                max_reports=args.max_reports,
                categories=args.categories
            )
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if not args.stats:
            downloader.show_stats()
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        sys.exit(1)
    finally:
        if downloader:
            try:
                downloader.cleanup()
            except:
                pass
    
    logger.info("\nâœ… ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()