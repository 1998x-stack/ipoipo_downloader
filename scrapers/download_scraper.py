"""
ä¸‹è½½é¡µçˆ¬è™« - Stage 3 & 4: è·å–ä¸‹è½½é“¾æ¥ï¼ˆä¿®å¤é˜²ç›—é“¾ï¼‰
"""
import re
import time
from typing import Optional, Dict
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from utils.logger import get_logger
from core.http_client import HTTPClient
from core.database import Database
from config.settings import DOWNLOAD_URL

logger = get_logger(__name__)


class DownloadScraper:
    """ä¸‹è½½é¡µçˆ¬è™«"""
    
    def __init__(self, http_client: HTTPClient, database: Database):
        self.client = http_client
        self.db = database
    
    def get_download_page_url(self, post_id: str) -> str:
        """Stage 3: å°†post URLè½¬æ¢ä¸ºdownload URL"""
        return DOWNLOAD_URL.format(post_id)
    
    def extract_zip_url(self, html: str, base_url: str = None) -> Optional[str]:
        """ä»HTMLä¸­æå–ZIPä¸‹è½½é“¾æ¥"""
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«.zipçš„é“¾æ¥
            zip_links = soup.find_all('a', href=re.compile(r'\.zip$', re.I))
            if zip_links:
                url = zip_links[0].get('href')
                return urljoin(base_url, url) if base_url else url
            
            # æ–¹æ³•2: æŸ¥æ‰¾ç‰¹å®šæ ·å¼çš„é“¾æ¥ï¼ˆæ ¹æ®ä½ æä¾›çš„HTMLç»“æ„ï¼‰
            # <a style="font-size: 12px; color: rgb(0, 102, 204);" href="...">xxx.zip</a>
            links = soup.find_all('a', style=re.compile(r'font-size.*color'))
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if '.zip' in href.lower() or '.zip' in text.lower():
                    return urljoin(base_url, href) if base_url else href
            
            # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾ï¼Œç­›é€‰æ–‡æœ¬åŒ…å«.zipçš„
            all_links = soup.find_all('a')
            for link in all_links:
                text = link.get_text(strip=True)
                if '.zip' in text.lower():
                    href = link.get('href', '')
                    if href:
                        return urljoin(base_url, href) if base_url else href
            
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ZIPä¸‹è½½é“¾æ¥")
            return None
            
        except Exception as e:
            logger.error(f"âŒ æå–ZIPé“¾æ¥å¤±è´¥: {e}")
            return None
    
    def build_download_headers(self, referer_url: str, zip_url: str) -> Dict[str, str]:
        """
        æ„å»ºç»•è¿‡é˜²ç›—é“¾çš„è¯·æ±‚å¤´
        
        å…³é”®ç‚¹ï¼š
        1. Refererå¿…é¡»æ˜¯ä¸‹è½½é¡µé¢URL
        2. å®Œæ•´çš„æµè§ˆå™¨User-Agent
        3. å…¶ä»–æµè§ˆå™¨å¸¸è§è¯·æ±‚å¤´
        """
        # è§£æZIP URLçš„åŸŸå
        parsed = urlparse(zip_url)
        origin = f"{parsed.scheme}://{parsed.netloc}"
        
        headers = {
            # æœ€å…³é”®ï¼šRefererå¿…é¡»æ˜¯ä¸‹è½½é¡µé¢
            'Referer': referer_url,
            
            # æµè§ˆå™¨æ ‡è¯†
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            
            # æ¥å—çš„å†…å®¹ç±»å‹
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            
            # è¿æ¥æ§åˆ¶
            'Connection': 'keep-alive',
            
            # ç¼“å­˜æ§åˆ¶
            'Cache-Control': 'max-age=0',
            
            # å‡çº§ä¸å®‰å…¨è¯·æ±‚
            'Upgrade-Insecure-Requests': '1',
            
            # Sec-Fetchç³»åˆ—ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
            
            # æµè§ˆå™¨ç‰¹å¾
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
        }
        
        return headers
    
    def download_zip_file(self, zip_url: str, referer_url: str, 
                         save_path: str, chunk_size: int = 8192) -> bool:
        """
        ä¸‹è½½ZIPæ–‡ä»¶ï¼ˆç»•è¿‡é˜²ç›—é“¾ï¼‰
        
        Args:
            zip_url: ZIPæ–‡ä»¶URL
            referer_url: æ¥æºé¡µé¢URLï¼ˆç”¨äºRefererï¼‰
            save_path: ä¿å­˜è·¯å¾„
            chunk_size: åˆ†å—ä¸‹è½½å¤§å°
        """
        try:
            # æ„å»ºè¯·æ±‚å¤´
            headers = self.build_download_headers(referer_url, zip_url)
            
            logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½: {zip_url}")
            logger.debug(f"ğŸ”‘ Referer: {referer_url}")
            
            # å‘é€è¯·æ±‚ï¼ˆä½¿ç”¨stream=Trueæ”¯æŒå¤§æ–‡ä»¶ï¼‰
            response = self.client.get(
                zip_url, 
                headers=headers,
                stream=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            # æ£€æŸ¥å“åº”
            if response.status_code == 403:
                logger.error(f"âŒ 403 Forbidden - é˜²ç›—é“¾æ‹¦æˆª")
                logger.error(f"   å¯èƒ½åŸå› ï¼š")
                logger.error(f"   1. Refererä¸æ­£ç¡®: {referer_url}")
                logger.error(f"   2. éœ€è¦CookieéªŒè¯")
                logger.error(f"   3. éœ€è¦å…ˆè®¿é—®ä¸‹è½½é¡µé¢")
                return False
            
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶å¤§å°
            total_size = int(response.headers.get('content-length', 0))
            
            # åˆ†å—ä¸‹è½½
            downloaded = 0
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        
                        # æ˜¾ç¤ºè¿›åº¦
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            if downloaded % (chunk_size * 100) == 0:  # æ¯800KBæ˜¾ç¤ºä¸€æ¬¡
                                logger.debug(f"   ä¸‹è½½è¿›åº¦: {progress:.1f}% ({downloaded}/{total_size})")
            
            logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {save_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return False
    
    def get_zip_download_url(self, post_id: str, visit_page_first: bool = True) -> Optional[str]:
        """
        Stage 4: è·å–ZIPæ–‡ä»¶çš„å®é™…ä¸‹è½½é“¾æ¥
        
        Args:
            post_id: æ–‡ç« ID
            visit_page_first: æ˜¯å¦å…ˆè®¿é—®é¡µé¢è·å–cookieï¼ˆé‡è¦ï¼ï¼‰
        """
        try:
            # è·å–ä¸‹è½½é¡µURL
            download_page_url = self.get_download_page_url(post_id)
            logger.info(f"ğŸ”— ä¸‹è½½é¡µ: {download_page_url}")
            
            # é‡è¦ï¼šå…ˆè®¿é—®ä¸‹è½½é¡µé¢ï¼Œå»ºç«‹ä¼šè¯å’Œcookie
            if visit_page_first:
                logger.debug("ğŸŒ å…ˆè®¿é—®ä¸‹è½½é¡µé¢ï¼ˆè·å–cookieï¼‰...")
                # ä½¿ç”¨æ™®é€šçš„æµè§ˆå™¨è¯·æ±‚å¤´
                page_headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9',
                }
                response = self.client.get(download_page_url, headers=page_headers)
                
                # çŸ­æš‚å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
                time.sleep(1)
            else:
                response = self.client.get(download_page_url)
            
            # æå–ZIPé“¾æ¥
            zip_url = self.extract_zip_url(response.text, base_url=download_page_url)
            
            if zip_url:
                logger.info(f"âœ… æ‰¾åˆ°ZIPé“¾æ¥: {zip_url}")
                
                # æ›´æ–°æ•°æ®åº“ï¼ŒåŒæ—¶ä¿å­˜referer
                self.db.update_report_download_url(post_id, zip_url)
                # æ³¨æ„ï¼šä½ å¯èƒ½éœ€è¦åœ¨æ•°æ®åº“ä¸­æ·»åŠ referer_urlå­—æ®µ
                # self.db.update_report_referer(post_id, download_page_url)
                
                return zip_url
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ZIPé“¾æ¥: {post_id}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è·å–ZIPé“¾æ¥å¤±è´¥: {post_id} - {e}")
            return None
    
    def process_report(self, post_id: str, download_file: bool = False, 
                      save_path: str = None) -> Optional[str]:
        """
        å¤„ç†å•ä¸ªæŠ¥å‘Šï¼šè·å–ä¸‹è½½é“¾æ¥å¹¶å¯é€‰ä¸‹è½½æ–‡ä»¶
        
        Args:
            post_id: æ–‡ç« ID
            download_file: æ˜¯å¦ç«‹å³ä¸‹è½½æ–‡ä»¶
            save_path: æ–‡ä»¶ä¿å­˜è·¯å¾„
        """
        # è·å–ä¸‹è½½é“¾æ¥
        zip_url = self.get_zip_download_url(post_id, visit_page_first=True)
        
        if not zip_url:
            return None
        
        # å¦‚æœéœ€è¦ä¸‹è½½æ–‡ä»¶
        if download_file and save_path:
            download_page_url = self.get_download_page_url(post_id)
            success = self.download_zip_file(zip_url, download_page_url, save_path)
            
            if not success:
                logger.warning("âš ï¸ ä¸‹è½½å¤±è´¥ï¼Œä½†å·²è·å–åˆ°é“¾æ¥")
        
        return zip_url
    
    def process_all_pending_reports(self, limit: int = 100):
        """å¤„ç†æ‰€æœ‰å¾…ä¸‹è½½çš„æŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info("ğŸ”— Stage 3 & 4: è·å–æ‰€æœ‰æŠ¥å‘Šçš„ä¸‹è½½é“¾æ¥")
        logger.info("=" * 60)
        
        reports = self.db.get_pending_reports(limit=limit)
        logger.info(f"ğŸ“Š å¾…å¤„ç†æŠ¥å‘Š: {len(reports)} ä¸ª")
        
        success_count = 0
        fail_count = 0
        
        for i, report in enumerate(reports, 1):
            post_id = report['post_id']
            title = report['title']
            
            logger.info(f"\n[{i}/{len(reports)}] {title}")
            
            zip_url = self.process_report(post_id)
            
            if zip_url:
                success_count += 1
                self.db.update_report_status(post_id, 'ready')
            else:
                fail_count += 1
                self.db.update_report_status(post_id, 'failed')
            
            # è¯·æ±‚é—´éš”ï¼Œé¿å…è¿‡å¿«
            if i < len(reports):
                time.sleep(2)
        
        logger.info(f"\n{'=' * 60}")
        logger.info(f"âœ… å¤„ç†å®Œæˆï¼")
        logger.info(f"  - æˆåŠŸ: {success_count}")
        logger.info(f"  - å¤±è´¥: {fail_count}")
        logger.info(f"{'=' * 60}")
    
    def test_download_with_referer(self, zip_url: str, referer_url: str):
        """
        æµ‹è¯•ï¼šä½¿ç”¨Refererä¸‹è½½æ–‡ä»¶
        ç”¨äºéªŒè¯é˜²ç›—é“¾ç»•è¿‡æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("ğŸ§ª æµ‹è¯•ä¸‹è½½ï¼ˆå¸¦Refererï¼‰")
        logger.info("=" * 60)
        
        headers = self.build_download_headers(referer_url, zip_url)
        
        logger.info(f"ZIP URL: {zip_url}")
        logger.info(f"Referer: {referer_url}")
        logger.info(f"\nè¯·æ±‚å¤´:")
        for key, value in headers.items():
            logger.info(f"  {key}: {value}")
        
        try:
            # åªè¯·æ±‚å¤´éƒ¨ï¼Œä¸ä¸‹è½½å®Œæ•´æ–‡ä»¶
            response = self.client.head(zip_url, headers=headers, timeout=30)
            
            logger.info(f"\nå“åº”çŠ¶æ€: {response.status_code}")
            logger.info(f"å“åº”å¤´:")
            for key, value in response.headers.items():
                logger.info(f"  {key}: {value}")
            
            if response.status_code == 200:
                logger.info("\nâœ… æˆåŠŸï¼å¯ä»¥ä¸‹è½½")
                file_size = response.headers.get('content-length', 'unknown')
                logger.info(f"æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                return True
            else:
                logger.error(f"\nâŒ å¤±è´¥ï¼çŠ¶æ€ç : {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"\nâŒ è¯·æ±‚å¤±è´¥: {e}")
            return False


if __name__ == "__main__":
    from core.proxy_manager import ProxyManager
    
    # åˆå§‹åŒ–
    pm = ProxyManager()
    pm.test_all_nodes()
    
    client = HTTPClient(use_proxy=True, proxy_manager=pm)
    db = Database()
    
    scraper = DownloadScraper(client, db)
    
    # æµ‹è¯•æ¡ˆä¾‹ï¼šä½ æä¾›çš„é“¾æ¥
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•é˜²ç›—é“¾ç»•è¿‡")
    print("=" * 60)
    
    test_zip_url = "https://ipo.ai-tag.cn/2023/12/202312251405085991116.zip"
    test_referer = "https://ipoipo.cn/xiazai/123456/"  # æ›¿æ¢ä¸ºå®é™…çš„ä¸‹è½½é¡µURL
    
    # æµ‹è¯•1: ä¸å¸¦Refererï¼ˆåº”è¯¥å¤±è´¥ï¼‰
    print("\næµ‹è¯•1: ä¸å¸¦Referer")
    try:
        response = client.head(test_zip_url, timeout=10)
        print(f"çŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        print(f"å¤±è´¥: {e}")
    
    # æµ‹è¯•2: å¸¦Refererï¼ˆåº”è¯¥æˆåŠŸï¼‰
    print("\næµ‹è¯•2: å¸¦Referer")
    scraper.test_download_with_referer(test_zip_url, test_referer)
    
    # æµ‹è¯•3: å®Œæ•´æµç¨‹
    print("\n" + "=" * 60)
    print("ğŸ”„ æµ‹è¯•å®Œæ•´ä¸‹è½½æµç¨‹")
    print("=" * 60)
    scraper.process_all_pending_reports(limit=3)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = db.get_stats()
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  - æ€»æŠ¥å‘Šæ•°: {stats['total_reports']}")
    print(f"  - æŒ‰çŠ¶æ€åˆ†å¸ƒ: {stats.get('reports_by_status', {})}")
    
    client.close()
    db.close()