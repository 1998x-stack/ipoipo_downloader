"""
åˆ†ç±»çˆ¬è™« - Stage 1: è·å–æ‰€æœ‰åˆ†ç±»ä¿¡æ¯
"""
from typing import List, Dict
from utils.logger import get_logger
from core.http_client import HTTPClient
from core.database import Database
from config.settings import CATEGORY_NAMES, CATEGORY_PAGE_URL

logger = get_logger(__name__)


class CategoryScraper:
    """åˆ†ç±»çˆ¬è™«"""
    
    def __init__(self, http_client: HTTPClient, database: Database):
        self.client = http_client
        self.db = database
    
    def scrape_all_categories(self) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰åˆ†ç±»"""
        logger.info("=" * 60)
        logger.info("ğŸ“š Stage 1: çˆ¬å–åˆ†ç±»åˆ—è¡¨")
        logger.info("=" * 60)
        
        categories = []
        
        for category_id, category_name in CATEGORY_NAMES.items():
            url = CATEGORY_PAGE_URL.format(category_id)
            
            category_data = {
                'category_id': category_id,
                'category_name': category_name,
                'url': url
            }
            
            categories.append(category_data)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.db.insert_category(category_id, category_name, url)
            logger.info(f"âœ… {category_name} ({category_id}): {url}")
        
        logger.info(f"\nâœ… å®Œæˆï¼å…± {len(categories)} ä¸ªåˆ†ç±»")
        return categories
    
    def get_categories_from_db(self) -> List[Dict]:
        """ä»æ•°æ®åº“è·å–åˆ†ç±»"""
        return self.db.get_all_categories()


if __name__ == "__main__":
    from core.proxy_manager import ProxyManager
    
    # åˆå§‹åŒ–
    pm = ProxyManager()
    pm.test_all_nodes()
    
    client = HTTPClient(use_proxy=True, proxy_manager=pm)
    db = Database()
    
    scraper = CategoryScraper(client, db)
    
    # çˆ¬å–åˆ†ç±»
    categories = scraper.scrape_all_categories()
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = db.get_stats()
    print(f"\nğŸ“Š ç»Ÿè®¡: {stats['total_categories']} ä¸ªåˆ†ç±»")
    
    client.close()
    db.close()