"""
åˆ—è¡¨çˆ¬è™« - Stage 2: çˆ¬å–æ¯ä¸ªåˆ†ç±»ä¸‹çš„æŠ¥å‘Šåˆ—è¡¨
"""
import re
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from utils.logger import get_logger
from core.http_client import HTTPClient
from core.database import Database
from config.settings import CATEGORY_PAGE_URL, CATEGORY_PAGE_PAGINATED

logger = get_logger(__name__)


class ListScraper:
    """åˆ—è¡¨é¡µçˆ¬è™«"""
    
    def __init__(self, http_client: HTTPClient, database: Database):
        self.client = http_client
        self.db = database
    
    def parse_report_card(self, card_element) -> Optional[Dict]:
        """è§£æå•ä¸ªæŠ¥å‘Šå¡ç‰‡"""
        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            h2 = card_element.find('h2', class_='multi-ellipsis')
            if not h2:
                return None
            
            link = h2.find('a')
            if not link:
                return None
            
            title = link.get('title', '').strip()
            detail_url = link.get('href', '').strip()
            
            # æå–post_idï¼ˆä»URLä¸­ï¼‰
            # ä¾‹å¦‚: https://ipoipo.cn/post/26028.html -> 26028
            match = re.search(r'/post/(\d+)\.html', detail_url)
            if not match:
                return None
            post_id = match.group(1)
            
            # æå–ç¼©ç•¥å›¾
            img = card_element.find('img', class_='img-cover')
            thumbnail_url = img.get('src', '') if img else ''
            
            # æå–ç®€ä»‹
            text_p = card_element.find('p', class_='text')
            description = text_p.get_text(strip=True) if text_p else ''
            
            # æå–æµè§ˆé‡å’Œå‘å¸ƒæ—¥æœŸ
            count_div = card_element.find('div', class_='count')
            view_count = 0
            publish_date = ''
            
            if count_div:
                view_span = count_div.find('span', class_='view-num')
                if view_span:
                    view_text = view_span.get_text(strip=True)
                    match = re.search(r'\d+', view_text)
                    if match:
                        view_count = int(match.group())
                
                edit_span = count_div.find('span', class_='edit')
                if edit_span:
                    publish_date = edit_span.get_text(strip=True)
            
            return {
                'post_id': post_id,
                'title': title,
                'detail_url': detail_url,
                'thumbnail_url': thumbnail_url,
                'description': description,
                'view_count': view_count,
                'publish_date': publish_date
            }
            
        except Exception as e:
            logger.error(f"âŒ è§£ææŠ¥å‘Šå¡ç‰‡å¤±è´¥: {e}")
            return None
    
    def scrape_page(self, url: str) -> List[Dict]:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        try:
            response = self.client.get(url)
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æŸ¥æ‰¾æ‰€æœ‰æŠ¥å‘Šå¡ç‰‡
            cards = soup.find_all('div', class_='wapost card')
            
            reports = []
            for card in cards:
                report = self.parse_report_card(card)
                if report:
                    reports.append(report)
            
            return reports
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–é¡µé¢å¤±è´¥: {url} - {e}")
            return []
    
    def scrape_category(self, category_id: str, category_name: str, 
                        max_pages: int = None) -> List[Dict]:
        """çˆ¬å–å•ä¸ªåˆ†ç±»çš„æ‰€æœ‰æŠ¥å‘Š"""
        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ“‘ çˆ¬å–åˆ†ç±»: {category_name} ({category_id})")
        logger.info(f"{'=' * 60}")
        
        all_reports = []
        page = 1
        
        while True:
            # æ„é€ URL
            if page == 1:
                url = CATEGORY_PAGE_URL.format(category_id)
            else:
                url = CATEGORY_PAGE_PAGINATED.format(category_id, page)
            
            logger.info(f"ğŸ“„ çˆ¬å–ç¬¬ {page} é¡µ: {url}")
            
            # çˆ¬å–é¡µé¢
            reports = self.scrape_page(url)
            
            if not reports:
                logger.info(f"âš ï¸ ç¬¬ {page} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            for report in reports:
                self.db.insert_report(
                    category_id=category_id,
                    post_id=report['post_id'],
                    title=report['title'],
                    detail_url=report['detail_url'],
                    thumbnail_url=report['thumbnail_url'],
                    view_count=report['view_count'],
                    publish_date=report['publish_date']
                )
            
            all_reports.extend(reports)
            logger.info(f"âœ… ç¬¬ {page} é¡µ: è·å– {len(reports)} ä¸ªæŠ¥å‘Š")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é¡µæ•°
            if max_pages and page >= max_pages:
                logger.info(f"âš ï¸ è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages}")
                break
            
            page += 1
        
        logger.info(f"\nâœ… å®Œæˆï¼{category_name} å…± {len(all_reports)} ä¸ªæŠ¥å‘Š")
        return all_reports
    
    def scrape_all_categories(self, max_pages_per_category: int = None):
        """çˆ¬å–æ‰€æœ‰åˆ†ç±»"""
        logger.info("=" * 60)
        logger.info("ğŸ“š Stage 2: çˆ¬å–æ‰€æœ‰åˆ†ç±»çš„æŠ¥å‘Šåˆ—è¡¨")
        logger.info("=" * 60)
        
        # ä»æ•°æ®åº“è·å–åˆ†ç±»
        categories = self.db.get_all_categories()
        
        total_reports = 0
        for i, category in enumerate(categories, 1):
            logger.info(f"\nè¿›åº¦: {i}/{len(categories)}")
            reports = self.scrape_category(
                category['category_id'],
                category['category_name'],
                max_pages=max_pages_per_category
            )
            total_reports += len(reports)
        
        logger.info(f"\n{'=' * 60}")
        logger.info(f"âœ… å…¨éƒ¨å®Œæˆï¼å…±çˆ¬å– {total_reports} ä¸ªæŠ¥å‘Š")
        logger.info(f"{'=' * 60}")


if __name__ == "__main__":
    from core.proxy_manager import ProxyManager
    
    # åˆå§‹åŒ–
    pm = ProxyManager()
    pm.test_all_nodes()
    
    client = HTTPClient(use_proxy=True, proxy_manager=pm)
    db = Database()
    
    scraper = ListScraper(client, db)
    
    # æµ‹è¯•ï¼šåªçˆ¬å–ç¬¬ä¸€ä¸ªåˆ†ç±»çš„å‰2é¡µ
    categories = db.get_all_categories()
    if categories:
        test_category = categories[0]
        scraper.scrape_category(
            test_category['category_id'],
            test_category['category_name'],
            max_pages=2
        )
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = db.get_stats()
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  - åˆ†ç±»æ•°: {stats['total_categories']}")
    print(f"  - æŠ¥å‘Šæ•°: {stats['total_reports']}")
    
    client.close()
    db.close()