"""
ä¸‹è½½é¡µçˆ¬è™« - ä¿®å¤ç‰ˆï¼ˆæ­£ç¡®å¤„ç†Tengine CDNé˜²ç›—é“¾ï¼‰

å…³é”®ä¿®å¤ï¼š
1. ä½¿ç”¨HTTPClientçš„Sessionä¿æŒcookies
2. æ­£ç¡®è®¾ç½®Refererï¼ˆå¿…é¡»æ˜¯ipoipo.cnåŸŸåï¼‰
3. å…ˆè®¿é—®ä¸‹è½½é¡µé¢å»ºç«‹ä¼šè¯ï¼Œå†ä¸‹è½½æ–‡ä»¶
"""
import re
import time
from typing import Optional, Dict, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from src.utils.logger import get_logger
from src.model.http_client import HTTPClient
from src.model.database import Database
from src.config.settings import DOWNLOAD_URL

logger = get_logger(__name__)


class DownloadScraper:
    """
    ä¸‹è½½é¡µçˆ¬è™« - å¤„ç†é˜²ç›—é“¾
    
    Tengine CDN é˜²ç›—é“¾æœºåˆ¶:
    - ä½¿ç”¨Referer ACLç™½åå•
    - åªå…è®¸æ¥è‡ª ipoipo.cn åŸŸåçš„Referer
    - ç›´æ¥è¯·æ±‚ZIP URLä¼šè¿”å›403
    
    è§£å†³æ–¹æ¡ˆ:
    - ä½¿ç”¨åŒä¸€ä¸ªSessionä¿æŒcookies
    - å…ˆè®¿é—®ä¸‹è½½é¡µé¢ï¼ˆè·å–cookiesï¼‰
    - ä¸‹è½½æ—¶è®¾ç½®Refererä¸ºä¸‹è½½é¡µé¢URL
    """
    
    def __init__(self, http_client: HTTPClient, database: Database):
        self.client = http_client
        self.db = database
    
    def get_download_page_url(self, post_id: str) -> str:
        """Stage 3: å°†post URLè½¬æ¢ä¸ºdownload URL"""
        return DOWNLOAD_URL.format(post_id)
    
    def extract_zip_url(self, html: str, base_url: str = None) -> Optional[str]:
        """
        ä»HTMLä¸­æå–ZIPä¸‹è½½é“¾æ¥
        
        æ”¯æŒå¤šç§åŒ¹é…æ–¹å¼ä»¥æé«˜æˆåŠŸç‡
        """
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            # æ–¹æ³•1: æŸ¥æ‰¾hrefåŒ…å«.zipçš„é“¾æ¥
            zip_links = soup.find_all('a', href=re.compile(r'\.zip$', re.I))
            if zip_links:
                url = zip_links[0].get('href')
                logger.debug(f"âœ… æ‰¾åˆ°ZIPé“¾æ¥ (æ–¹æ³•1-hrefåŒ¹é…): {url}")
                return urljoin(base_url, url) if base_url else url
            
            # æ–¹æ³•2: æŸ¥æ‰¾ç‰¹å®šæ ·å¼çš„é“¾æ¥
            links = soup.find_all('a', style=re.compile(r'font-size.*color'))
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if '.zip' in href.lower() or '.zip' in text.lower():
                    logger.debug(f"âœ… æ‰¾åˆ°ZIPé“¾æ¥ (æ–¹æ³•2-æ ·å¼åŒ¹é…): {href}")
                    return urljoin(base_url, href) if base_url else href
            
            # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾ï¼Œç­›é€‰æ–‡æœ¬åŒ…å«.zipçš„
            all_links = soup.find_all('a')
            for link in all_links:
                text = link.get_text(strip=True)
                if '.zip' in text.lower():
                    href = link.get('href', '')
                    if href:
                        logger.debug(f"âœ… æ‰¾åˆ°ZIPé“¾æ¥ (æ–¹æ³•3-æ–‡æœ¬åŒ¹é…): {href}")
                        return urljoin(base_url, href) if base_url else href
            
            # æ–¹æ³•4: æ­£åˆ™åŒ¹é…HTMLä¸­çš„ZIP URL
            zip_pattern = r'https?://[^\s<>"\']+\.zip'
            matches = re.findall(zip_pattern, html, re.I)
            if matches:
                logger.debug(f"âœ… æ‰¾åˆ°ZIPé“¾æ¥ (æ–¹æ³•4-æ­£åˆ™åŒ¹é…): {matches[0]}")
                return matches[0]
            
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ZIPä¸‹è½½é“¾æ¥")
            return None
            
        except Exception as e:
            logger.error(f"âŒ æå–ZIPé“¾æ¥å¤±è´¥: {e}")
            return None
    
    def visit_download_page(self, download_page_url: str) -> Tuple[bool, Optional[str]]:
        """
        è®¿é—®ä¸‹è½½é¡µé¢
        
        è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼š
        1. å»ºç«‹ä¼šè¯ï¼Œè·å–å¿…è¦çš„cookies
        2. HTTPClientçš„Sessionä¼šè‡ªåŠ¨ä¿å­˜cookies
        
        Returns:
            (success, html_content)
        """
        logger.info(f"ğŸ“„ è®¿é—®ä¸‹è½½é¡µé¢: {download_page_url}")
        
        try:
            response = self.client.get(
                download_page_url,
                timeout=30
            )
            
            # æ‰“å°è·å–åˆ°çš„cookiesï¼ˆè°ƒè¯•ç”¨ï¼‰
            cookies = self.client.get_cookies()
            if cookies:
                logger.debug(f"ğŸª è·å–åˆ°çš„Cookies: {list(cookies.keys())}")
            
            return True, response.text
            
        except Exception as e:
            logger.error(f"âŒ è®¿é—®ä¸‹è½½é¡µé¢å¤±è´¥: {e}")
            return False, None
    
    def get_zip_download_url(self, post_id: str) -> Tuple[Optional[str], Optional[str]]:
        """
        è·å–ZIPæ–‡ä»¶çš„ä¸‹è½½é“¾æ¥
        
        Args:
            post_id: æ–‡ç« ID
            
        Returns:
            (zip_url, download_page_url) - åŒæ—¶è¿”å›refererç”¨äºåç»­ä¸‹è½½
        """
        try:
            # è·å–ä¸‹è½½é¡µURL
            download_page_url = self.get_download_page_url(post_id)
            
            # è®¿é—®ä¸‹è½½é¡µé¢ï¼ˆå»ºç«‹sessionï¼Œè·å–cookiesï¼‰
            success, html = self.visit_download_page(download_page_url)
            if not success or not html:
                return None, None
            
            # æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼šçŸ­æš‚å»¶è¿Ÿ
            time.sleep(0.5)
            
            # æå–ZIPé“¾æ¥
            zip_url = self.extract_zip_url(html, base_url=download_page_url)
            
            if zip_url:
                logger.info(f"âœ… æ‰¾åˆ°ZIPé“¾æ¥: {zip_url}")
                
                # æ›´æ–°æ•°æ®åº“
                self.db.update_report_download_url(post_id, zip_url)
                
                # è¿”å›zip_urlå’Œrefererï¼ˆä¸‹è½½é¡µé¢URLï¼‰
                return zip_url, download_page_url
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ZIPé“¾æ¥: {post_id}")
                return None, None
                
        except Exception as e:
            logger.error(f"âŒ è·å–ZIPé“¾æ¥å¤±è´¥: {post_id} - {e}")
            return None, None
    
    def download_zip_file(self, zip_url: str, referer_url: str, 
                         save_path: str) -> bool:
        """
        ä¸‹è½½ZIPæ–‡ä»¶ï¼ˆä½¿ç”¨HTTPClientçš„download_fileæ–¹æ³•ï¼‰
        
        å…³é”®ï¼šreferer_url å¿…é¡»æ˜¯ ipoipo.cn åŸŸåçš„ä¸‹è½½é¡µé¢URL
        
        Args:
            zip_url: ZIPæ–‡ä»¶URL (å¦‚ https://ipo.ai-tag.cn/2025/04/xxx.zip)
            referer_url: æ¥æºé¡µé¢URL (å¦‚ https://ipoipo.cn/xiazai/123456/)
            save_path: ä¿å­˜è·¯å¾„
        """
        logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ZIPæ–‡ä»¶")
        logger.info(f"   URL: {zip_url}")
        logger.info(f"   Referer: {referer_url}")
        logger.info(f"   ä¿å­˜è·¯å¾„: {save_path}")
        
        # ä½¿ç”¨HTTPClientçš„ä¸‹è½½æ–¹æ³•ï¼ˆä¼šæºå¸¦Sessionä¸­çš„cookiesï¼‰
        return self.client.download_file(
            url=zip_url,
            save_path=save_path,
            referer=referer_url,  # å…³é”®ï¼šRefererå¿…é¡»æ˜¯ipoipo.cnåŸŸå
            timeout=300
        )
    
    def process_report(self, post_id: str, download_file: bool = False, 
                      save_path: str = None) -> Optional[str]:
        """
        å¤„ç†å•ä¸ªæŠ¥å‘Šï¼šè·å–ä¸‹è½½é“¾æ¥å¹¶å¯é€‰ä¸‹è½½æ–‡ä»¶
        
        Args:
            post_id: æ–‡ç« ID
            download_file: æ˜¯å¦ç«‹å³ä¸‹è½½æ–‡ä»¶
            save_path: æ–‡ä»¶ä¿å­˜è·¯å¾„
            
        Returns:
            zip_url æˆ– None
        """
        # è·å–ä¸‹è½½é“¾æ¥ï¼ˆåŒæ—¶ä¼šè®¿é—®ä¸‹è½½é¡µé¢å»ºç«‹sessionï¼‰
        zip_url, download_page_url = self.get_zip_download_url(post_id)
        
        if not zip_url:
            return None
        
        # å¦‚æœéœ€è¦ä¸‹è½½æ–‡ä»¶
        if download_file and save_path:
            # çŸ­æš‚å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            time.sleep(1)
            
            success = self.download_zip_file(zip_url, download_page_url, save_path)
            
            if not success:
                logger.warning("âš ï¸ ä¸‹è½½å¤±è´¥ï¼Œä½†å·²è·å–åˆ°é“¾æ¥")
        
        return zip_url
    
    def process_all_pending_reports(self, limit: int = 100):
        """å¤„ç†æ‰€æœ‰å¾…è·å–ä¸‹è½½é“¾æ¥çš„æŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info("ğŸ”— Stage 3 & 4: è·å–æ‰€æœ‰æŠ¥å‘Šçš„ä¸‹è½½é“¾æ¥")
        logger.info("=" * 60)
        
        reports = self.db.get_pending_reports(limit=limit)
        logger.info(f"ğŸ“Š å¾…å¤„ç†æŠ¥å‘Š: {len(reports)} ä¸ª")
        
        success_count = 0
        fail_count = 0
        
        for i, report in enumerate(reports, 1):
            post_id = report['post_id']
            title = report['title']
            
            logger.info(f"\n[{i}/{len(reports)}] {title}")
            
            zip_url, _ = self.get_zip_download_url(post_id)
            
            if zip_url:
                success_count += 1
                self.db.update_report_status(post_id, 'ready')
            else:
                fail_count += 1
                self.db.update_report_status(post_id, 'failed')
            
            # è¯·æ±‚é—´éš”ï¼Œé¿å…è¿‡å¿«
            if i < len(reports):
                time.sleep(2)
        
        logger.info(f"\n{'=' * 60}")
        logger.info(f"âœ… å¤„ç†å®Œæˆï¼")
        logger.info(f"  - æˆåŠŸ: {success_count}")
        logger.info(f"  - å¤±è´¥: {fail_count}")
        logger.info(f"{'=' * 60}")
    
    def test_download_with_referer(self, zip_url: str, referer_url: str) -> bool:
        """
        æµ‹è¯•ï¼šä½¿ç”¨Refererä¸‹è½½æ–‡ä»¶
        ç”¨äºéªŒè¯é˜²ç›—é“¾ç»•è¿‡æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("ğŸ§ª æµ‹è¯•ä¸‹è½½ï¼ˆå¸¦Refererï¼‰")
        logger.info("=" * 60)
        
        logger.info(f"ZIP URL: {zip_url}")
        logger.info(f"Referer: {referer_url}")
        
        try:
            # å…ˆè®¿é—®refereré¡µé¢å»ºç«‹session
            logger.info("ğŸ“„ å…ˆè®¿é—®æ¥æºé¡µé¢...")
            self.client.get(referer_url, timeout=10)
            time.sleep(1)
            
            # ä½¿ç”¨HEADè¯·æ±‚æµ‹è¯•ï¼ˆä¸ä¸‹è½½å®Œæ•´æ–‡ä»¶ï¼‰
            headers = {
                'Referer': referer_url,
                'Sec-Fetch-Site': 'cross-site',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Dest': 'document',
            }
            
            response = self.client.head(zip_url, headers=headers, timeout=30)
            
            logger.info(f"\nå“åº”çŠ¶æ€: {response.status_code}")
            logger.info(f"å“åº”å¤´:")
            for key in ['Content-Type', 'Content-Length', 'X-Tengine-Error']:
                if key in response.headers:
                    logger.info(f"  {key}: {response.headers[key]}")
            
            if response.status_code == 200:
                logger.info("\nâœ… æµ‹è¯•æˆåŠŸï¼å¯ä»¥ä¸‹è½½")
                file_size = response.headers.get('Content-Length', 'unknown')
                logger.info(f"æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                return True
            elif response.status_code == 403:
                logger.error("\nâŒ æµ‹è¯•å¤±è´¥ï¼403 Forbidden")
                tengine_error = response.headers.get('X-Tengine-Error', '')
                if tengine_error:
                    logger.error(f"   X-Tengine-Error: {tengine_error}")
                return False
            else:
                logger.warning(f"\nâš ï¸ æ„å¤–çš„çŠ¶æ€ç : {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"\nâŒ è¯·æ±‚å¤±è´¥: {e}")
            return False


if __name__ == "__main__":
    from src.model.proxy_manager import ProxyManager
    
    # åˆå§‹åŒ–
    pm = ProxyManager()
    pm.test_all_nodes()
    
    # ä½¿ç”¨éšæœºèŠ‚ç‚¹çš„ä»£ç†URL
    pm.select_random()
    proxy_url = f"http://127.0.0.1:{pm.local_port}"
    
    client = HTTPClient(use_proxy=True, proxy_url=proxy_url)
    
    # å‡è®¾Databaseç±»å­˜åœ¨
    # db = Database()
    # scraper = DownloadScraper(client, db)
    
    # æµ‹è¯•é˜²ç›—é“¾ç»•è¿‡
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•é˜²ç›—é“¾ç»•è¿‡")
    print("=" * 60)
    
    test_zip_url = "https://ipo.ai-tag.cn/2025/04/202504291200327477262.zip"
    test_referer = "https://ipoipo.cn/xiazai/123456/"
    
    # æµ‹è¯•1: ä¸å¸¦Refererï¼ˆåº”è¯¥å¤±è´¥ï¼‰
    print("\næµ‹è¯•1: ä¸å¸¦Referer")
    try:
        response = client.head(test_zip_url, timeout=10)
        print(f"çŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        print(f"å¤±è´¥: {e}")
    
    # æµ‹è¯•2: å¸¦Refererï¼ˆåº”è¯¥æˆåŠŸï¼‰
    print("\næµ‹è¯•2: å¸¦Referer")
    print("å…ˆè®¿é—®æ¥æºé¡µé¢...")
    try:
        client.get(test_referer, timeout=10)
        time.sleep(1)
        
        headers = {'Referer': test_referer, 'Sec-Fetch-Site': 'cross-site'}
        response = client.head(test_zip_url, headers=headers, timeout=10)
        print(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            print("âœ… æˆåŠŸç»•è¿‡é˜²ç›—é“¾ï¼")
    except Exception as e:
        print(f"å¤±è´¥: {e}")
    
    client.close()