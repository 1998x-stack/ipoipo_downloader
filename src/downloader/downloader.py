"""
ä¸‹è½½å™¨ - è´Ÿè´£å®é™…ä¸‹è½½æŠ¥å‘Šæ–‡ä»¶ï¼ˆå®Œæ•´ç‰ˆï¼‰

åŠŸèƒ½ï¼š
1. ä¸‹è½½ ZIP æ–‡ä»¶ï¼ˆç»•è¿‡é˜²ç›—é“¾ï¼‰
2. è‡ªåŠ¨è§£å‹ ZIP æ–‡ä»¶
3. è‡ªåŠ¨é‡å‘½åæ–‡æ¡£ï¼ˆæ—¶é—´æˆ³ + æŠ¥å‘Šæ ‡é¢˜ï¼‰
4. æ”¯æŒæ‰¹é‡ä¸‹è½½å’Œé‡è¯•
"""
import os
import time
from pathlib import Path
from typing import Optional, List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
from src.utils.logger import get_logger
from src.model.http_client import HTTPClient
from src.model.database import Database
from src.downloader.file_manager import FileManager
from src.config.settings import DOWNLOAD_URL

logger = get_logger(__name__)


class Downloader:
    """
    æŠ¥å‘Šä¸‹è½½å™¨
    
    å®Œæ•´æµç¨‹ï¼š
    1. è·å–å¾…ä¸‹è½½æŠ¥å‘Šä¿¡æ¯
    2. è®¿é—®ä¸‹è½½é¡µé¢ï¼ˆå»ºç«‹sessionï¼Œè·å–cookiesï¼‰
    3. ä¸‹è½½ ZIP æ–‡ä»¶ï¼ˆä½¿ç”¨æ­£ç¡®çš„ Referer ç»•è¿‡é˜²ç›—é“¾ï¼‰
    4. è§£å‹ ZIP æ–‡ä»¶
    5. é‡å‘½åæ–‡æ¡£æ–‡ä»¶ï¼ˆæ—¶é—´æˆ³ + æŠ¥å‘Šæ ‡é¢˜ï¼‰
    6. å¯é€‰ï¼šåˆ é™¤åŸå§‹ ZIP æ–‡ä»¶
    """
    
    def __init__(self, http_client: HTTPClient, database: Database, 
                 file_manager: FileManager, 
                 auto_extract: bool = True,
                 auto_rename: bool = True,
                 keep_zip: bool = False,
                 proxy_switch_callback=None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            http_client: HTTPå®¢æˆ·ç«¯
            database: æ•°æ®åº“å®ä¾‹
            file_manager: æ–‡ä»¶ç®¡ç†å™¨
            auto_extract: æ˜¯å¦è‡ªåŠ¨è§£å‹
            auto_rename: æ˜¯å¦è‡ªåŠ¨é‡å‘½åæ–‡æ¡£
            keep_zip: æ˜¯å¦ä¿ç•™ZIPæ–‡ä»¶
            proxy_switch_callback: ä»£ç†åˆ‡æ¢å›è°ƒå‡½æ•°ï¼ˆ403æ—¶è‡ªåŠ¨è°ƒç”¨ï¼‰
        """
        self.client = http_client
        self.db = database
        self.fm = file_manager
        self.auto_extract = auto_extract
        self.auto_rename = auto_rename
        self.keep_zip = keep_zip
        self.proxy_switch_callback = proxy_switch_callback
        
        # è¿ç»­å¤±è´¥è®¡æ•°ï¼ˆç”¨äºè§¦å‘ä»£ç†åˆ‡æ¢ï¼‰
        self._consecutive_failures = 0
        self._max_failures_before_switch = 2  # è¿ç»­å¤±è´¥2æ¬¡ååˆ‡æ¢ä»£ç†
    
    def get_download_page_url(self, post_id: str) -> str:
        """è·å–ä¸‹è½½é¡µé¢URLï¼ˆç”¨ä½œRefererï¼‰"""
        return DOWNLOAD_URL.format(post_id)
    
    def _try_switch_proxy(self, reason: str = "download failed") -> bool:
        """
        å°è¯•åˆ‡æ¢ä»£ç†èŠ‚ç‚¹
        
        Args:
            reason: åˆ‡æ¢åŸå› 
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ‡æ¢
        """
        if self.proxy_switch_callback:
            logger.warning(f"âš ï¸ {reason}ï¼Œå°è¯•åˆ‡æ¢ä»£ç†èŠ‚ç‚¹...")
            if self.proxy_switch_callback():
                self._consecutive_failures = 0
                time.sleep(2)  # åˆ‡æ¢åç­‰å¾…ä¸€ä¸‹
                return True
        return False
    
    def _handle_download_failure(self, is_403: bool = False) -> bool:
        """
        å¤„ç†ä¸‹è½½å¤±è´¥
        
        Args:
            is_403: æ˜¯å¦ä¸º403é”™è¯¯
            
        Returns:
            æ˜¯å¦åº”è¯¥é‡è¯•
        """
        self._consecutive_failures += 1
        
        # 403é”™è¯¯ç«‹å³å°è¯•åˆ‡æ¢ä»£ç†
        if is_403:
            logger.error("âŒ 403 Forbidden - é˜²ç›—é“¾æ‹¦æˆª")
            return self._try_switch_proxy("é­é‡403é˜²ç›—é“¾æ‹¦æˆª")
        
        # è¿ç»­å¤±è´¥å¤šæ¬¡ååˆ‡æ¢ä»£ç†
        if self._consecutive_failures >= self._max_failures_before_switch:
            return self._try_switch_proxy(f"è¿ç»­å¤±è´¥{self._consecutive_failures}æ¬¡")
        
        return False
    
    def _reset_failure_count(self):
        """é‡ç½®å¤±è´¥è®¡æ•°"""
        self._consecutive_failures = 0
    
    def download_report(self, report: Dict, force: bool = False, 
                       retry_on_403: bool = True) -> bool:
        """
        ä¸‹è½½å•ä¸ªæŠ¥å‘Šï¼ˆå®Œæ•´æµç¨‹ï¼šä¸‹è½½ -> è§£å‹ -> é‡å‘½åï¼‰
        
        Args:
            report: æŠ¥å‘Šä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«:
                - post_id: æ–‡ç« ID
                - title: æŠ¥å‘Šæ ‡é¢˜
                - download_url: ZIPä¸‹è½½é“¾æ¥
                - category_name: åˆ†ç±»åç§°
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            retry_on_403: é‡åˆ°403æ—¶æ˜¯å¦åˆ‡æ¢ä»£ç†é‡è¯•
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        post_id = report['post_id']
        title = report['title']
        zip_url = report.get('download_url')
        category_name = report.get('category_name', 'unknown')
        
        logger.info(f"\n{'=' * 50}")
        logger.info(f"ğŸ“„ å¤„ç†æŠ¥å‘Š: {title}")
        logger.info(f"{'=' * 50}")
        
        # æ£€æŸ¥ZIP URLæ˜¯å¦å­˜åœ¨
        if not zip_url:
            logger.warning(f"âš ï¸ æ²¡æœ‰ä¸‹è½½é“¾æ¥: {post_id}")
            return False
        
        # ç”Ÿæˆä¿å­˜è·¯å¾„
        zip_filename = self._extract_filename_from_url(zip_url)
        save_path = self.fm.get_report_path(category_name, zip_filename)
        save_path_obj = Path(save_path)
        
        logger.info(f"ğŸ“‚ åˆ†ç±»: {category_name}")
        logger.info(f"ğŸ“ ä¿å­˜è·¯å¾„: {save_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if not force and save_path_obj.exists():
            file_size = save_path_obj.stat().st_size
            if file_size > 1024:  # å¤§äº1KBè®¤ä¸ºæ˜¯æœ‰æ•ˆæ–‡ä»¶
                logger.info(f"â­ï¸ ZIPæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
                
                # å¦‚æœéœ€è¦è§£å‹ä½†è¿˜æ²¡è§£å‹ï¼Œæ‰§è¡Œè§£å‹
                if self.auto_extract:
                    self._extract_and_rename(save_path_obj, title)
                
                self._reset_failure_count()
                return True
        
        # æ„å»ºä¸‹è½½é¡µé¢URLï¼ˆç”¨ä½œRefererï¼‰
        download_page_url = self.get_download_page_url(post_id)
        
        # æœ€å¤šé‡è¯•æ¬¡æ•°ï¼ˆåŒ…æ‹¬åˆ‡æ¢ä»£ç†åçš„é‡è¯•ï¼‰
        max_attempts = 3 if retry_on_403 else 1
        
        for attempt in range(1, max_attempts + 1):
            try:
                if attempt > 1:
                    logger.info(f"ğŸ”„ ç¬¬ {attempt} æ¬¡å°è¯•...")
                
                # Step 1: è®¿é—®ä¸‹è½½é¡µé¢ï¼ˆå»ºç«‹sessionï¼Œè·å–cookiesï¼‰
                logger.info(f"ğŸ“„ Step 1: è®¿é—®ä¸‹è½½é¡µé¢...")
                logger.debug(f"   URL: {download_page_url}")
                self.client.get(download_page_url, timeout=30)
                
                # çŸ­æš‚å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
                time.sleep(1)
                
                # Step 2: ä¸‹è½½ZIPæ–‡ä»¶ï¼ˆä½¿ç”¨ä¸‹è½½é¡µé¢URLä½œä¸ºRefererï¼‰
                logger.info(f"ğŸ“¥ Step 2: ä¸‹è½½ZIPæ–‡ä»¶...")
                logger.debug(f"   URL: {zip_url}")
                logger.debug(f"   Referer: {download_page_url}")
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                self.fm.ensure_directory(save_path)
                
                success = self.client.download_file(
                    url=zip_url,
                    save_path=save_path,
                    referer=download_page_url,  # å…³é”®ï¼šRefererå¿…é¡»æ˜¯ipoipo.cnåŸŸå
                    timeout=300
                )
                
                if not success:
                    # æ£€æŸ¥æ˜¯å¦ä¸º403é”™è¯¯ï¼ˆéœ€è¦ä»HTTPClientè·å–ï¼‰
                    is_403 = getattr(self.client, '_last_status_code', None) == 403
                    
                    if is_403 and attempt < max_attempts:
                        if self._handle_download_failure(is_403=True):
                            continue  # åˆ‡æ¢ä»£ç†æˆåŠŸï¼Œé‡è¯•
                    
                    self._handle_download_failure(is_403=False)
                    self.db.update_report_status(post_id, 'failed')
                    logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {title}")
                    return False
                
                # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
                if not save_path_obj.exists() or save_path_obj.stat().st_size < 1024:
                    logger.error(f"âŒ ä¸‹è½½çš„æ–‡ä»¶æ— æ•ˆæˆ–å¤ªå°")
                    
                    if attempt < max_attempts:
                        if self._handle_download_failure(is_403=False):
                            continue
                    
                    self.db.update_report_status(post_id, 'failed')
                    return False
                
                file_size = self.fm.format_size(save_path_obj.stat().st_size)
                logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {zip_filename} ({file_size})")
                
                # Step 3: è§£å‹å’Œé‡å‘½å
                if self.auto_extract:
                    extract_success = self._extract_and_rename(save_path_obj, title)
                    if not extract_success:
                        logger.warning(f"âš ï¸ è§£å‹å¤±è´¥ï¼Œä½†ZIPæ–‡ä»¶å·²ä¿å­˜")
                
                # æ›´æ–°æ•°æ®åº“çŠ¶æ€
                self.db.update_report_status(post_id, 'downloaded')
                self.db.update_report_local_path(post_id, save_path)
                
                # é‡ç½®å¤±è´¥è®¡æ•°
                self._reset_failure_count()
                
                logger.info(f"âœ… å¤„ç†å®Œæˆ: {title}")
                return True
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥: {title} - {e}")
                import traceback
                logger.debug(traceback.format_exc())
                
                # å°è¯•åˆ‡æ¢ä»£ç†é‡è¯•
                if attempt < max_attempts:
                    if self._handle_download_failure(is_403=False):
                        continue
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        self.db.update_report_status(post_id, 'failed')
        return False
    
    def _extract_filename_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–æ–‡ä»¶å"""
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        
        if filename.endswith('.zip'):
            return filename
        
        # å¦‚æœURLä¸­æ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶åï¼Œç”Ÿæˆä¸€ä¸ª
        return f"report_{int(time.time())}.zip"
    
    def _extract_and_rename(self, zip_path: Path, report_title: str) -> bool:
        """
        è§£å‹ZIPæ–‡ä»¶å¹¶é‡å‘½åæ–‡æ¡£
        
        Args:
            zip_path: ZIPæ–‡ä»¶è·¯å¾„
            report_title: æŠ¥å‘Šæ ‡é¢˜ï¼ˆç”¨äºé‡å‘½åï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"ğŸ“¦ Step 3: è§£å‹å’Œé‡å‘½å...")
        
        try:
            # éªŒè¯ZIPæ–‡ä»¶
            if not self.fm.validate_and_fix_zip(zip_path):
                logger.error(f"âŒ ZIPæ–‡ä»¶æ— æ•ˆï¼Œè·³è¿‡è§£å‹")
                return False
            
            # è§£å‹åˆ°åŒä¸€ç›®å½•
            extract_dir = zip_path.parent
            
            # è°ƒç”¨FileManagerçš„extract_zipæ–¹æ³•
            # è¿™ä¼šè‡ªåŠ¨ï¼š
            # 1. ä»ZIPæ–‡ä»¶åæå–æ—¶é—´æˆ³
            # 2. è§£å‹æ‰€æœ‰æ–‡ä»¶
            # 3. é‡å‘½åæ–‡æ¡£æ–‡ä»¶ï¼ˆæ—¶é—´æˆ³ + æŠ¥å‘Šæ ‡é¢˜ï¼‰
            result = self.fm.extract_zip(
                zip_path=zip_path,
                extract_to=extract_dir,
                report_title=report_title,
                auto_rename=self.auto_rename
            )
            
            if result is None:
                logger.error(f"âŒ è§£å‹å¤±è´¥")
                return False
            
            # è·å–è§£å‹åçš„æ–‡æ¡£æ–‡ä»¶
            doc_files = self.fm.get_extracted_files(
                extract_dir, 
                extensions=self.fm.DOCUMENT_EXTENSIONS
            )
            
            if doc_files:
                logger.info(f"ğŸ“‹ è§£å‹åçš„æ–‡æ¡£æ–‡ä»¶:")
                for doc in doc_files:
                    size = self.fm.format_size(doc.stat().st_size)
                    logger.info(f"   - {doc.name} ({size})")
            
            # æ¸…ç†ZIPæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            if not self.keep_zip:
                self.fm.cleanup_zip(zip_path, keep_zip=False)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ è§£å‹è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return False
    
    def download_reports_by_category(self, category_id: str, 
                                     max_reports: int = None,
                                     force: bool = False) -> Dict[str, int]:
        """
        ä¸‹è½½æŒ‡å®šåˆ†ç±»çš„æŠ¥å‘Š
        
        Args:
            category_id: åˆ†ç±»ID
            max_reports: æœ€å¤§ä¸‹è½½æ•°é‡
            force: å¼ºåˆ¶é‡æ–°ä¸‹è½½
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯ {success: int, failed: int, skipped: int}
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ“‚ å¼€å§‹ä¸‹è½½åˆ†ç±» {category_id} çš„æŠ¥å‘Š")
        logger.info("=" * 60)
        
        # è·å–å¾…ä¸‹è½½æŠ¥å‘Š
        reports = self.db.get_reports_by_category(category_id, status='ready')
        
        if max_reports:
            reports = reports[:max_reports]
        
        logger.info(f"ğŸ“Š å¾…ä¸‹è½½æŠ¥å‘Š: {len(reports)} ä¸ª")
        
        if not reports:
            logger.info("âœ… æ²¡æœ‰å¾…ä¸‹è½½çš„æŠ¥å‘Š")
            return {'success': 0, 'failed': 0, 'skipped': 0}
        
        return self._download_sequential(reports, force)
    
    def download_all_reports(self, max_reports: int = None, 
                            force: bool = False,
                            use_concurrent: bool = False,
                            max_workers: int = 3) -> Dict[str, int]:
        """
        ä¸‹è½½æ‰€æœ‰å¾…ä¸‹è½½çš„æŠ¥å‘Š
        
        Args:
            max_reports: æœ€å¤§ä¸‹è½½æ•°é‡
            force: å¼ºåˆ¶é‡æ–°ä¸‹è½½
            use_concurrent: æ˜¯å¦ä½¿ç”¨å¹¶å‘ä¸‹è½½
            max_workers: å¹¶å‘æ•°é‡
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("=" * 60)
        logger.info("ğŸ“¥ å¼€å§‹ä¸‹è½½æ‰€æœ‰æŠ¥å‘Š")
        logger.info("=" * 60)
        
        # è·å–æ‰€æœ‰readyçŠ¶æ€çš„æŠ¥å‘Š
        reports = self.db.get_ready_reports(limit=max_reports or 1000)
        logger.info(f"ğŸ“Š å¾…ä¸‹è½½æŠ¥å‘Š: {len(reports)} ä¸ª")
        
        if not reports:
            logger.info("âœ… æ²¡æœ‰å¾…ä¸‹è½½çš„æŠ¥å‘Š")
            return {'success': 0, 'failed': 0, 'skipped': 0}
        
        if use_concurrent:
            return self._download_concurrent(reports, force, max_workers)
        else:
            return self._download_sequential(reports, force)
    
    def _download_sequential(self, reports: List[Dict], 
                            force: bool = False) -> Dict[str, int]:
        """é¡ºåºä¸‹è½½"""
        stats = {'success': 0, 'failed': 0, 'skipped': 0}
        total = len(reports)
        
        for i, report in enumerate(reports, 1):
            logger.info(f"\n[{i}/{total}] å¼€å§‹å¤„ç†...")
            
            try:
                success = self.download_report(report, force=force)
                
                if success:
                    stats['success'] += 1
                else:
                    stats['failed'] += 1
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
                stats['failed'] += 1
            
            # ä¸‹è½½é—´éš”ï¼ˆé‡è¦ï¼šé¿å…è§¦å‘é˜²æŠ¤ï¼‰
            if i < total:
                logger.info("â³ ç­‰å¾…2ç§’åç»§ç»­...")
                time.sleep(2)
        
        self._print_stats(stats)
        return stats
    
    def _download_concurrent(self, reports: List[Dict], 
                            force: bool = False,
                            max_workers: int = 3) -> Dict[str, int]:
        """
        å¹¶å‘ä¸‹è½½
        
        æ³¨æ„ï¼šå¹¶å‘ä¸‹è½½æ—¶å¯èƒ½è§¦å‘æ›´å¤šé˜²æŠ¤ï¼Œå»ºè®®è°¨æ…ä½¿ç”¨
        """
        logger.warning("âš ï¸ å¹¶å‘ä¸‹è½½å¯èƒ½è§¦å‘é˜²ç›—é“¾ï¼Œå¦‚å¤±è´¥è¯·æ”¹ç”¨é¡ºåºä¸‹è½½")
        
        stats = {'success': 0, 'failed': 0, 'skipped': 0}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_report = {
                executor.submit(self.download_report, report, force): report 
                for report in reports
            }
            
            for future in as_completed(future_to_report):
                report = future_to_report[future]
                try:
                    success = future.result()
                    if success:
                        stats['success'] += 1
                    else:
                        stats['failed'] += 1
                except Exception as e:
                    logger.error(f"âŒ ä¸‹è½½å¼‚å¸¸: {report['title']} - {e}")
                    stats['failed'] += 1
        
        self._print_stats(stats)
        return stats
    
    def _print_stats(self, stats: Dict[str, int]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡")
        logger.info(f"  - æˆåŠŸ: {stats['success']}")
        logger.info(f"  - å¤±è´¥: {stats['failed']}")
        logger.info(f"  - è·³è¿‡: {stats['skipped']}")
        total = stats['success'] + stats['failed'] + stats['skipped']
        if total > 0:
            success_rate = (stats['success'] / total) * 100
            logger.info(f"  - æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"{'=' * 60}")
    
    def retry_failed_downloads(self, max_reports: int = None) -> Dict[str, int]:
        """é‡è¯•å¤±è´¥çš„ä¸‹è½½"""
        logger.info("=" * 60)
        logger.info("ğŸ”„ é‡è¯•å¤±è´¥çš„ä¸‹è½½")
        logger.info("=" * 60)
        
        # è·å–å¤±è´¥çš„æŠ¥å‘Š
        reports = self.db.get_failed_reports(limit=max_reports or 100)
        logger.info(f"ğŸ“Š å¾…é‡è¯•æŠ¥å‘Š: {len(reports)} ä¸ª")
        
        if not reports:
            logger.info("âœ… æ²¡æœ‰å¤±è´¥çš„ä¸‹è½½éœ€è¦é‡è¯•")
            return {'success': 0, 'failed': 0, 'skipped': 0}
        
        # é‡ç½®çŠ¶æ€ä¸ºready
        for report in reports:
            self.db.update_report_status(report['post_id'], 'ready')
        
        # é‡æ–°ä¸‹è½½ï¼ˆå¼ºåˆ¶æ¨¡å¼ï¼‰
        return self._download_sequential(reports, force=True)
    
    def extract_downloaded_zips(self, category_name: str = None, 
                               max_files: int = None) -> Dict[str, int]:
        """
        è§£å‹å·²ä¸‹è½½ä½†æœªè§£å‹çš„ZIPæ–‡ä»¶
        
        Args:
            category_name: åˆ†ç±»åç§°ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰ï¼‰
            max_files: æœ€å¤§å¤„ç†æ•°é‡
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("=" * 60)
        logger.info("ğŸ“¦ è§£å‹å·²ä¸‹è½½çš„ZIPæ–‡ä»¶")
        logger.info("=" * 60)
        
        # è·å–å·²ä¸‹è½½çš„æŠ¥å‘Š
        reports = self.db.get_downloaded_reports(limit=max_files or 1000)
        
        if category_name:
            reports = [r for r in reports if r.get('category_name') == category_name]
        
        logger.info(f"ğŸ“Š å¾…å¤„ç†æŠ¥å‘Š: {len(reports)} ä¸ª")
        
        stats = {'success': 0, 'failed': 0, 'skipped': 0}
        
        for i, report in enumerate(reports, 1):
            title = report['title']
            local_path = report.get('local_path')
            
            if not local_path:
                logger.warning(f"âš ï¸ [{i}] æ²¡æœ‰æœ¬åœ°è·¯å¾„: {title}")
                stats['skipped'] += 1
                continue
            
            zip_path = Path(local_path)
            
            if not zip_path.exists():
                logger.warning(f"âš ï¸ [{i}] æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
                stats['skipped'] += 1
                continue
            
            logger.info(f"\n[{i}/{len(reports)}] å¤„ç†: {title}")
            
            success = self._extract_and_rename(zip_path, title)
            
            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1
        
        self._print_stats(stats)
        return stats


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("Downloaderæ¨¡å—æµ‹è¯•")
    print("è¯·é€šè¿‡main.pyè¿è¡Œå®Œæ•´æµ‹è¯•")
    print("\nç¤ºä¾‹å‘½ä»¤ï¼š")
    print("  python main.py --stage4 --max-reports 5")
    print("  python main.py --retry --max-reports 10")